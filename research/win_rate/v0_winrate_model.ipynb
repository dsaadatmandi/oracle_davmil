{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"02cd0c8a-8c17-4b6d-82c0-b7f4bc34a56f\"\n",
    "\n",
    "\n",
    "# url = \"https://api.opendota.com/api/matches/6745683521?api_key=\"\n",
    "# url = \"https://api.opendota.com/api/heroStats?api_key=\"\n",
    "# url = \"https://api.opendota.com/api/publicMatches?api_key=\"\n",
    "\n",
    "# resp = requests.get(url=url)\n",
    "# data = json.loads(resp.content)\n",
    "# matches  = pd.DataFrame(data)[\"match_id\"].values\n",
    "\n",
    "# drafts = []\n",
    "# outcome = []\n",
    "\n",
    "# for match in matches:\n",
    "#     _query = f\"https://api.opendota.com/api/matches/{match}?api_key=\"\n",
    "#     resp = requests.get(_query)\n",
    "#     assert resp.ok\n",
    "#     data = json.loads(resp.content)\n",
    "#     drafts.append(data[\"picks_bans\"])\n",
    "#     outcome.append(int(data[\"radiant_win\"]*1.0))\n",
    "#     time.sleep(1)\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# def process_raw_drafts(raw_draft):\n",
    "#     \"\"\"\n",
    "#     Takes raw draft data and givens 2 arrays, one with radiant's draft\n",
    "#     and the other with dire's\n",
    "#     \"\"\"\n",
    "\n",
    "#     draft_radiant = [_d[\"hero_id\"] for _d in raw_draft if _d[\"is_pick\"] and not _d[\"team\"]]\n",
    "#     draft_dire = [_d[\"hero_id\"] for _d in raw_draft if _d[\"is_pick\"] and _d[\"team\"]]\n",
    "\n",
    "#     return np.array(draft_radiant + draft_dire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download basic hero stats that we can use for embedding\n",
    "\n",
    "_hero_embedding_query = \"https://api.opendota.com/api/heroStats?api_key=\"\n",
    "resp = requests.get(_hero_embedding_query)\n",
    "assert resp.ok\n",
    "stats = pd.DataFrame(json.loads(resp.content)).set_index(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['name', 'localized_name', 'primary_attr', 'attack_type', 'roles', 'img',\n",
       "       'icon', 'base_health', 'base_health_regen', 'base_mana',\n",
       "       'base_mana_regen', 'base_armor', 'base_mr', 'base_attack_min',\n",
       "       'base_attack_max', 'base_str', 'base_agi', 'base_int', 'str_gain',\n",
       "       'agi_gain', 'int_gain', 'attack_range', 'projectile_speed',\n",
       "       'attack_rate', 'move_speed', 'turn_rate', 'cm_enabled', 'legs',\n",
       "       'hero_id', 'turbo_picks', 'turbo_wins', 'pro_ban', 'pro_win',\n",
       "       'pro_pick', '1_pick', '1_win', '2_pick', '2_win', '3_pick', '3_win',\n",
       "       '4_pick', '4_win', '5_pick', '5_win', '6_pick', '6_win', '7_pick',\n",
       "       '7_win', '8_pick', '8_win', 'null_pick', 'null_win'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Carry</th>\n",
       "      <th>Disabler</th>\n",
       "      <th>Durable</th>\n",
       "      <th>Escape</th>\n",
       "      <th>Initiator</th>\n",
       "      <th>Jungler</th>\n",
       "      <th>Nuker</th>\n",
       "      <th>Pusher</th>\n",
       "      <th>Support</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>123 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Carry  Disabler  Durable  Escape  Initiator  Jungler  Nuker  Pusher  \\\n",
       "id                                                                         \n",
       "1        1         0        0       1          0        0      1       0   \n",
       "2        1         1        1       0          1        1      0       0   \n",
       "3        0         1        1       0          0        0      1       0   \n",
       "4        1         1        0       0          1        1      1       0   \n",
       "5        0         1        0       0          0        1      1       0   \n",
       "..     ...       ...      ...     ...        ...      ...    ...     ...   \n",
       "128      0         1        0       1          0        0      1       0   \n",
       "129      1         1        1       0          1        0      0       0   \n",
       "135      1         0        1       0          0        0      0       0   \n",
       "136      1         1        0       1          1        0      0       0   \n",
       "137      0         1        1       0          1        0      0       0   \n",
       "\n",
       "     Support  \n",
       "id            \n",
       "1          0  \n",
       "2          0  \n",
       "3          1  \n",
       "4          0  \n",
       "5          1  \n",
       "..       ...  \n",
       "128        1  \n",
       "129        0  \n",
       "135        0  \n",
       "136        1  \n",
       "137        0  \n",
       "\n",
       "[123 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get roles embedding for each hero\n",
    "roles_encoding = stats[\"roles\"].str.join(\"|\").str.get_dummies()\n",
    "roles_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attack_type_Melee</th>\n",
       "      <th>attack_type_Ranged</th>\n",
       "      <th>primary_attr_agi</th>\n",
       "      <th>primary_attr_int</th>\n",
       "      <th>primary_attr_str</th>\n",
       "      <th>Carry</th>\n",
       "      <th>Disabler</th>\n",
       "      <th>Durable</th>\n",
       "      <th>Escape</th>\n",
       "      <th>Initiator</th>\n",
       "      <th>Jungler</th>\n",
       "      <th>Nuker</th>\n",
       "      <th>Pusher</th>\n",
       "      <th>Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>123 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     attack_type_Melee  attack_type_Ranged  primary_attr_agi  \\\n",
       "0                    1                   0                 1   \n",
       "1                    1                   0                 0   \n",
       "2                    0                   1                 0   \n",
       "3                    1                   0                 1   \n",
       "4                    0                   1                 0   \n",
       "..                 ...                 ...               ...   \n",
       "118                  0                   1                 0   \n",
       "119                  1                   0                 0   \n",
       "120                  1                   0                 0   \n",
       "121                  1                   0                 0   \n",
       "122                  1                   0                 0   \n",
       "\n",
       "     primary_attr_int  primary_attr_str  Carry  Disabler  Durable  Escape  \\\n",
       "0                   0                 0      1         0        0       1   \n",
       "1                   0                 1      1         1        1       0   \n",
       "2                   1                 0      0         1        1       0   \n",
       "3                   0                 0      1         1        0       0   \n",
       "4                   1                 0      0         1        0       0   \n",
       "..                ...               ...    ...       ...      ...     ...   \n",
       "118                 0                 1      0         1        0       1   \n",
       "119                 0                 1      1         1        1       0   \n",
       "120                 0                 1      1         0        1       0   \n",
       "121                 0                 1      1         1        0       1   \n",
       "122                 0                 1      0         1        1       0   \n",
       "\n",
       "     Initiator  Jungler  Nuker  Pusher  Support  \n",
       "0            0        0      1       0        0  \n",
       "1            1        1      0       0        0  \n",
       "2            0        0      1       0        1  \n",
       "3            1        1      1       0        0  \n",
       "4            0        1      1       0        1  \n",
       "..         ...      ...    ...     ...      ...  \n",
       "118          0        0      1       0        1  \n",
       "119          1        0      0       0        0  \n",
       "120          0        0      0       0        0  \n",
       "121          1        0      0       0        1  \n",
       "122          1        0      0       0        0  \n",
       "\n",
       "[123 rows x 14 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# basic embedding: roles + attack_type + primary_attribute\n",
    "\n",
    "embedding = pd.concat([pd.get_dummies(stats[[\"attack_type\", \"primary_attr\"]]), roles_encoding], axis = 1).reset_index(drop=True)\n",
    "embedding  # we have our embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in dataset of drafts match outcomes\n",
    "\n",
    "dataset = pd.read_pickle(\"dota.pickle\")\n",
    "radiant_win, radiant_draft, dire_draft, _, _, num_matches, num_heroes = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "heroes_list = np.unique(np.concatenate([radiant_draft, dire_draft], axis=0).flatten())\n",
    "embedding = embedding.loc[np.arange(num_heroes)].values\n",
    "\n",
    "# build the draft vector described in paper, a (num_heroes x 1) vector with +1 in the ith entry if the ith hero is picked by radiant, -1 if picked by the dire, 0 otherwise.\n",
    "drafts = list()\n",
    "empty_draft = np.zeros((num_heroes, ))\n",
    "\n",
    "for idx in range(num_matches):\n",
    "    temp = empty_draft.copy()\n",
    "    temp[radiant_draft[idx]] = 1\n",
    "    temp[dire_draft[idx]] = -1\n",
    "\n",
    "    drafts.append(temp)\n",
    "\n",
    "drafts = np.asarray(drafts, dtype=np.int64)\n",
    "\n",
    "# our dataset of drafts\n",
    "assert drafts.shape == (num_matches, num_heroes)\n",
    "\n",
    "# embedding dataset for each match\n",
    "K = drafts @ embedding\n",
    "\n",
    "# combined datset, both embedding + hero indication\n",
    "X = np.concatenate((K, drafts), axis=1)\n",
    "assert X.shape == (num_matches, embedding.shape[1] + num_heroes)\n",
    "\n",
    "y = radiant_win.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train, val and test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "idx_train, idx_test = train_test_split(np.arange(num_matches), test_size=0.2, train_size=0.8, shuffle=True)\n",
    "idx_train, idx_val = train_test_split(idx_train, train_size=0.75, test_size=0.25, shuffle=True)\n",
    "\n",
    "# keep the split points for X and K the same\n",
    "X_train, X_val, X_test = X[idx_train], X[idx_val], X[idx_test]\n",
    "K_train, K_val, K_test = K[idx_train], K[idx_val], K[idx_test]\n",
    "y_train, y_val, y_test = y[idx_train], y[idx_val], y[idx_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000000.0, penalty=&#x27;l1&#x27;, solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000000.0, penalty=&#x27;l1&#x27;, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000000.0, penalty='l1', solver='liblinear')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit benchmark Logistic Regression model with only embeddings\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "base_model = LogisticRegression(penalty=\"l1\", solver=\"liblinear\", max_iter=1e6)\n",
    "base_model.fit(K_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for training datset: 0.5574040176514499\n",
      "Accuracy score for validation datset: 0.5585807082717861\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy score for training datset: {accuracy_score(y_train, base_model.predict(K_train))}\")\n",
    "print(f\"Accuracy score for validation datset: {accuracy_score(y_val, base_model.predict(K_val))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(penalty=&#x27;l1&#x27;, solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(penalty=&#x27;l1&#x27;, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(penalty='l1', solver='liblinear')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit larger LR model with hero specific coefs\n",
    "\n",
    "hero_model = LogisticRegression(penalty=\"l1\", solver=\"liblinear\")\n",
    "hero_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for training datset: 0.6390749619538517\n",
      "Accuracy score for validation datset: 0.6386354750956538\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy score for training datset: {accuracy_score(y_train, hero_model.predict(X_train))}\")\n",
    "print(f\"Accuracy score for validation datset: {accuracy_score(y_val, hero_model.predict(X_val))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding hero specific features increases model accuracy by ~14%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a deep neural network\n",
    "\"\"\"\n",
    "The motivation for a DNN is that we hope to to try and capture non-linear combinations of heroes\n",
    "to improve performance. For example a DNN will allow us to try and find pairs or triplets of heroes\n",
    "that have 'better than additive' synergy.\n",
    "\"\"\"\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "feature_number = embedding.shape[1] + num_heroes\n",
    "\n",
    "# class NeuralNetwork(nn.Module):\n",
    "#     def __init__(self) -> None:\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.network = nn.Sequential(\n",
    "#             nn.Linear(feature_number, 200),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(200, 50),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(50, 1)\n",
    "#             # nn.Softmax()\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         return self.network(x).flatten()\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(feature_number, 40),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(40, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 1)\n",
    "            # nn.Softmax()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x).flatten()\n",
    "\n",
    "nn_model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer our numpy data to torch tensors/dataloaders\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "TRAIN_BATCH_SIZE = 50\n",
    "TEST_BATCH_SIZE = VAL_BATCH_SIZE = 10_000\n",
    "\n",
    "\n",
    "X_train_tensor = torch.Tensor(X_train)\n",
    "K_train_tensor = torch.Tensor(K_train)\n",
    "y_train_tensor = torch.Tensor(y_train)\n",
    "\n",
    "X_val_tensor = torch.Tensor(X_val)\n",
    "K_val_tensor = torch.Tensor(K_val)\n",
    "y_val_tensor = torch.Tensor(y_val)\n",
    "\n",
    "X_test_tensor = torch.Tensor(X_test)\n",
    "K_test_tensor = torch.Tensor(K_test)\n",
    "y_test_tensor = torch.Tensor(y_test)\n",
    "\n",
    "full_train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "full_val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "full_test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "test_dataloader = DataLoader(full_test_dataset, batch_size=TEST_BATCH_SIZE)\n",
    "val_dataloader = DataLoader(full_val_dataset, batch_size=VAL_BATCH_SIZE)\n",
    "train_dataloader = DataLoader(full_train_dataset, batch_size=TRAIN_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparams, loss and optimizer\n",
    "\n",
    "learning_rate = 5e-5\n",
    "epochs = 1\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(nn_model.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(params=nn_model.parameters(), lr=1e-4)\n",
    "# eval_fn = accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "class TrainingResults(NamedTuple):\n",
    "    train_evaluation_history: np.array\n",
    "    validation_evaultation_history: np.array\n",
    "\n",
    "\n",
    "def train_and_validation_loop(model, training_dataloader, validation_dataloader, loss_function, optimization_engine, evaluation_function) -> TrainingResults:\n",
    "    size = len(training_dataloader.dataset)\n",
    "    training_eval_history = list()\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        pred = model(X)\n",
    "        loss = loss_function(pred, y)\n",
    "\n",
    "        optimization_engine.zero_grad()\n",
    "        loss.backward()\n",
    "        optimization_engine.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print(f\"Step [{batch * len(X):>5d}/{size:>5d}]\")\n",
    "\n",
    "            # current training metrics\n",
    "            pred = torch.sigmoid(pred) > 0.5\n",
    "            eval_training = evaluation_function(y, pred)\n",
    "            training_eval_history.append(eval_training)\n",
    "            print(f\"Training evaluation metric: {evaluation_function(y, pred):.2f}\")\n",
    "            print(f\"Training loss: {loss.item():.2f}\")\n",
    "            \n",
    "        if batch % 1000 == 0:\n",
    "            # test on our validation dataset\n",
    "            total_validation_accuracy = 0\n",
    "            with torch.no_grad():\n",
    "                for X_val, y_val in validation_dataloader:\n",
    "                    pred_val = model(X_val)\n",
    "                    pred_val = torch.sigmoid(pred_val) > 0.5\n",
    "                    total_validation_accuracy += evaluation_function(y_val, pred_val)\n",
    "\n",
    "            print(f\"Validation evalutaion metric: {total_validation_accuracy / len(val_dataloader):.2f}\")\n",
    "\n",
    "    plt.plot(training_eval_history)\n",
    "                \n",
    "\n",
    "def torch_accuracy_score(y_true, y_pred):\n",
    "    sum = torch.sum(y_true == y_pred).item()\n",
    "    return sum / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [    0/1833957]\n",
      "Training evaluation metric: 0.60\n",
      "Training loss: 87.22\n",
      "Validation evalutaion metric: 0.61\n",
      "Step [ 5000/1833957]\n",
      "Training evaluation metric: 0.62\n",
      "Training loss: 103.21\n",
      "Step [10000/1833957]\n",
      "Training evaluation metric: 0.66\n",
      "Training loss: 112.30\n",
      "Step [15000/1833957]\n",
      "Training evaluation metric: 0.58\n",
      "Training loss: 105.28\n",
      "Step [20000/1833957]\n",
      "Training evaluation metric: 0.68\n",
      "Training loss: 122.42\n",
      "Step [25000/1833957]\n",
      "Training evaluation metric: 0.70\n",
      "Training loss: 115.10\n",
      "Step [30000/1833957]\n",
      "Training evaluation metric: 0.62\n",
      "Training loss: 87.17\n",
      "Step [35000/1833957]\n",
      "Training evaluation metric: 0.52\n",
      "Training loss: 93.84\n",
      "Step [40000/1833957]\n",
      "Training evaluation metric: 0.74\n",
      "Training loss: 118.58\n",
      "Step [45000/1833957]\n",
      "Training evaluation metric: 0.70\n",
      "Training loss: 120.01\n",
      "Step [50000/1833957]\n",
      "Training evaluation metric: 0.56\n",
      "Training loss: 101.24\n",
      "Validation evalutaion metric: 0.61\n",
      "Step [55000/1833957]\n",
      "Training evaluation metric: 0.68\n",
      "Training loss: 124.61\n",
      "Step [60000/1833957]\n",
      "Training evaluation metric: 0.64\n",
      "Training loss: 115.63\n",
      "Step [65000/1833957]\n",
      "Training evaluation metric: 0.44\n",
      "Training loss: 77.42\n",
      "Step [70000/1833957]\n",
      "Training evaluation metric: 0.70\n",
      "Training loss: 114.65\n",
      "Step [75000/1833957]\n",
      "Training evaluation metric: 0.54\n",
      "Training loss: 101.30\n",
      "Step [80000/1833957]\n",
      "Training evaluation metric: 0.66\n",
      "Training loss: 116.43\n",
      "Step [85000/1833957]\n",
      "Training evaluation metric: 0.54\n",
      "Training loss: 110.19\n",
      "Step [90000/1833957]\n",
      "Training evaluation metric: 0.64\n",
      "Training loss: 99.72\n",
      "Step [95000/1833957]\n",
      "Training evaluation metric: 0.56\n",
      "Training loss: 97.86\n",
      "Step [100000/1833957]\n",
      "Training evaluation metric: 0.62\n",
      "Training loss: 88.84\n",
      "Validation evalutaion metric: 0.62\n",
      "Step [105000/1833957]\n",
      "Training evaluation metric: 0.72\n",
      "Training loss: 119.46\n",
      "Step [110000/1833957]\n",
      "Training evaluation metric: 0.56\n",
      "Training loss: 96.96\n",
      "Step [115000/1833957]\n",
      "Training evaluation metric: 0.54\n",
      "Training loss: 110.59\n",
      "Step [120000/1833957]\n",
      "Training evaluation metric: 0.62\n",
      "Training loss: 101.16\n",
      "Step [125000/1833957]\n",
      "Training evaluation metric: 0.68\n",
      "Training loss: 112.17\n",
      "Step [130000/1833957]\n",
      "Training evaluation metric: 0.64\n",
      "Training loss: 97.35\n",
      "Step [135000/1833957]\n",
      "Training evaluation metric: 0.64\n",
      "Training loss: 108.24\n",
      "Step [140000/1833957]\n",
      "Training evaluation metric: 0.68\n",
      "Training loss: 103.81\n",
      "Step [145000/1833957]\n",
      "Training evaluation metric: 0.56\n",
      "Training loss: 92.53\n",
      "Step [150000/1833957]\n",
      "Training evaluation metric: 0.66\n",
      "Training loss: 116.86\n",
      "Validation evalutaion metric: 0.61\n",
      "Step [155000/1833957]\n",
      "Training evaluation metric: 0.68\n",
      "Training loss: 111.92\n",
      "Step [160000/1833957]\n",
      "Training evaluation metric: 0.66\n",
      "Training loss: 106.54\n",
      "Step [165000/1833957]\n",
      "Training evaluation metric: 0.78\n",
      "Training loss: 132.77\n",
      "Step [170000/1833957]\n",
      "Training evaluation metric: 0.56\n",
      "Training loss: 97.15\n",
      "Step [175000/1833957]\n",
      "Training evaluation metric: 0.66\n",
      "Training loss: 103.33\n",
      "Step [180000/1833957]\n",
      "Training evaluation metric: 0.54\n",
      "Training loss: 89.22\n",
      "Step [185000/1833957]\n",
      "Training evaluation metric: 0.66\n",
      "Training loss: 116.40\n",
      "Step [190000/1833957]\n",
      "Training evaluation metric: 0.44\n",
      "Training loss: 82.59\n",
      "Step [195000/1833957]\n",
      "Training evaluation metric: 0.56\n",
      "Training loss: 96.59\n",
      "Step [200000/1833957]\n",
      "Training evaluation metric: 0.64\n",
      "Training loss: 116.63\n",
      "Validation evalutaion metric: 0.61\n",
      "Step [205000/1833957]\n",
      "Training evaluation metric: 0.74\n",
      "Training loss: 133.17\n",
      "Step [210000/1833957]\n",
      "Training evaluation metric: 0.54\n",
      "Training loss: 119.89\n",
      "Step [215000/1833957]\n",
      "Training evaluation metric: 0.72\n",
      "Training loss: 105.79\n",
      "Step [220000/1833957]\n",
      "Training evaluation metric: 0.70\n",
      "Training loss: 98.69\n",
      "Step [225000/1833957]\n",
      "Training evaluation metric: 0.64\n",
      "Training loss: 120.99\n",
      "Step [230000/1833957]\n",
      "Training evaluation metric: 0.62\n",
      "Training loss: 92.77\n",
      "Step [235000/1833957]\n",
      "Training evaluation metric: 0.56\n",
      "Training loss: 79.60\n",
      "Step [240000/1833957]\n",
      "Training evaluation metric: 0.58\n",
      "Training loss: 91.89\n",
      "Step [245000/1833957]\n",
      "Training evaluation metric: 0.64\n",
      "Training loss: 124.18\n",
      "Step [250000/1833957]\n",
      "Training evaluation metric: 0.54\n",
      "Training loss: 100.00\n",
      "Validation evalutaion metric: 0.61\n",
      "Step [255000/1833957]\n",
      "Training evaluation metric: 0.68\n",
      "Training loss: 128.07\n",
      "Step [260000/1833957]\n",
      "Training evaluation metric: 0.52\n",
      "Training loss: 96.56\n",
      "Step [265000/1833957]\n",
      "Training evaluation metric: 0.62\n",
      "Training loss: 100.15\n",
      "Step [270000/1833957]\n",
      "Training evaluation metric: 0.72\n",
      "Training loss: 106.70\n",
      "Step [275000/1833957]\n",
      "Training evaluation metric: 0.58\n",
      "Training loss: 117.03\n",
      "Step [280000/1833957]\n",
      "Training evaluation metric: 0.64\n",
      "Training loss: 96.96\n",
      "Step [285000/1833957]\n",
      "Training evaluation metric: 0.62\n",
      "Training loss: 120.94\n",
      "Step [290000/1833957]\n",
      "Training evaluation metric: 0.66\n",
      "Training loss: 115.71\n",
      "Step [295000/1833957]\n",
      "Training evaluation metric: 0.58\n",
      "Training loss: 79.41\n",
      "Step [300000/1833957]\n",
      "Training evaluation metric: 0.60\n",
      "Training loss: 96.23\n",
      "Validation evalutaion metric: 0.61\n",
      "Step [305000/1833957]\n",
      "Training evaluation metric: 0.74\n",
      "Training loss: 126.92\n",
      "Step [310000/1833957]\n",
      "Training evaluation metric: 0.50\n",
      "Training loss: 82.34\n",
      "Step [315000/1833957]\n",
      "Training evaluation metric: 0.66\n",
      "Training loss: 112.26\n",
      "Step [320000/1833957]\n",
      "Training evaluation metric: 0.56\n",
      "Training loss: 96.21\n",
      "Step [325000/1833957]\n",
      "Training evaluation metric: 0.80\n",
      "Training loss: 117.19\n",
      "Step [330000/1833957]\n",
      "Training evaluation metric: 0.60\n",
      "Training loss: 109.17\n",
      "Step [335000/1833957]\n",
      "Training evaluation metric: 0.62\n",
      "Training loss: 112.93\n",
      "Step [340000/1833957]\n",
      "Training evaluation metric: 0.62\n",
      "Training loss: 112.81\n",
      "Step [345000/1833957]\n",
      "Training evaluation metric: 0.56\n",
      "Training loss: 112.69\n",
      "Step [350000/1833957]\n",
      "Training evaluation metric: 0.58\n",
      "Training loss: 82.42\n",
      "Validation evalutaion metric: 0.60\n",
      "Step [355000/1833957]\n",
      "Training evaluation metric: 0.58\n",
      "Training loss: 100.32\n",
      "Step [360000/1833957]\n",
      "Training evaluation metric: 0.74\n",
      "Training loss: 97.77\n",
      "Step [365000/1833957]\n",
      "Training evaluation metric: 0.64\n",
      "Training loss: 124.41\n",
      "Step [370000/1833957]\n",
      "Training evaluation metric: 0.62\n",
      "Training loss: 95.51\n",
      "Step [375000/1833957]\n",
      "Training evaluation metric: 0.58\n",
      "Training loss: 80.22\n",
      "Step [380000/1833957]\n",
      "Training evaluation metric: 0.58\n",
      "Training loss: 78.84\n",
      "Step [385000/1833957]\n",
      "Training evaluation metric: 0.66\n",
      "Training loss: 111.81\n",
      "Step [390000/1833957]\n",
      "Training evaluation metric: 0.68\n",
      "Training loss: 107.83\n",
      "Step [395000/1833957]\n",
      "Training evaluation metric: 0.68\n",
      "Training loss: 120.35\n",
      "Step [400000/1833957]\n",
      "Training evaluation metric: 0.56\n",
      "Training loss: 88.68\n",
      "Validation evalutaion metric: 0.61\n",
      "Step [405000/1833957]\n",
      "Training evaluation metric: 0.64\n",
      "Training loss: 94.88\n",
      "Step [410000/1833957]\n",
      "Training evaluation metric: 0.72\n",
      "Training loss: 123.17\n",
      "Step [415000/1833957]\n",
      "Training evaluation metric: 0.70\n",
      "Training loss: 124.02\n",
      "Step [420000/1833957]\n",
      "Training evaluation metric: 0.64\n",
      "Training loss: 129.23\n",
      "Step [425000/1833957]\n",
      "Training evaluation metric: 0.54\n",
      "Training loss: 92.32\n",
      "Step [430000/1833957]\n",
      "Training evaluation metric: 0.58\n",
      "Training loss: 113.51\n",
      "Step [435000/1833957]\n",
      "Training evaluation metric: 0.60\n",
      "Training loss: 100.20\n",
      "Step [440000/1833957]\n",
      "Training evaluation metric: 0.70\n",
      "Training loss: 119.61\n",
      "Step [445000/1833957]\n",
      "Training evaluation metric: 0.64\n",
      "Training loss: 117.10\n",
      "Step [450000/1833957]\n",
      "Training evaluation metric: 0.64\n",
      "Training loss: 121.00\n",
      "Validation evalutaion metric: 0.60\n",
      "Step [455000/1833957]\n",
      "Training evaluation metric: 0.50\n",
      "Training loss: 92.91\n",
      "Step [460000/1833957]\n",
      "Training evaluation metric: 0.56\n",
      "Training loss: 88.61\n",
      "Step [465000/1833957]\n",
      "Training evaluation metric: 0.56\n",
      "Training loss: 113.41\n",
      "Step [470000/1833957]\n",
      "Training evaluation metric: 0.64\n",
      "Training loss: 103.86\n",
      "Step [475000/1833957]\n",
      "Training evaluation metric: 0.66\n",
      "Training loss: 120.43\n",
      "Step [480000/1833957]\n",
      "Training evaluation metric: 0.60\n",
      "Training loss: 116.32\n",
      "Step [485000/1833957]\n",
      "Training evaluation metric: 0.64\n",
      "Training loss: 118.72\n",
      "Step [490000/1833957]\n",
      "Training evaluation metric: 0.60\n",
      "Training loss: 87.06\n",
      "Step [495000/1833957]\n",
      "Training evaluation metric: 0.54\n",
      "Training loss: 97.11\n",
      "Step [500000/1833957]\n",
      "Training evaluation metric: 0.70\n",
      "Training loss: 131.31\n",
      "Validation evalutaion metric: 0.61\n",
      "Step [505000/1833957]\n",
      "Training evaluation metric: 0.56\n",
      "Training loss: 79.14\n",
      "Step [510000/1833957]\n",
      "Training evaluation metric: 0.52\n",
      "Training loss: 106.73\n",
      "Step [515000/1833957]\n",
      "Training evaluation metric: 0.48\n",
      "Training loss: 102.51\n",
      "Step [520000/1833957]\n",
      "Training evaluation metric: 0.76\n",
      "Training loss: 127.91\n",
      "Step [525000/1833957]\n",
      "Training evaluation metric: 0.60\n",
      "Training loss: 96.23\n",
      "Step [530000/1833957]\n",
      "Training evaluation metric: 0.56\n",
      "Training loss: 90.01\n",
      "Step [535000/1833957]\n",
      "Training evaluation metric: 0.46\n",
      "Training loss: 81.20\n",
      "Step [540000/1833957]\n",
      "Training evaluation metric: 0.54\n",
      "Training loss: 76.02\n",
      "Step [545000/1833957]\n",
      "Training evaluation metric: 0.66\n",
      "Training loss: 113.34\n",
      "Step [550000/1833957]\n",
      "Training evaluation metric: 0.68\n",
      "Training loss: 120.59\n",
      "Validation evalutaion metric: 0.61\n",
      "Step [555000/1833957]\n",
      "Training evaluation metric: 0.50\n",
      "Training loss: 93.05\n",
      "Step [560000/1833957]\n",
      "Training evaluation metric: 0.62\n",
      "Training loss: 112.42\n",
      "Step [565000/1833957]\n",
      "Training evaluation metric: 0.50\n",
      "Training loss: 80.59\n",
      "Step [570000/1833957]\n",
      "Training evaluation metric: 0.58\n",
      "Training loss: 101.40\n",
      "Step [575000/1833957]\n",
      "Training evaluation metric: 0.58\n",
      "Training loss: 79.97\n",
      "Step [580000/1833957]\n",
      "Training evaluation metric: 0.62\n",
      "Training loss: 99.97\n",
      "Step [585000/1833957]\n",
      "Training evaluation metric: 0.70\n",
      "Training loss: 107.23\n",
      "Step [590000/1833957]\n",
      "Training evaluation metric: 0.42\n",
      "Training loss: 45.44\n",
      "Step [595000/1833957]\n",
      "Training evaluation metric: 0.70\n",
      "Training loss: 128.91\n",
      "Step [600000/1833957]\n",
      "Training evaluation metric: 0.64\n",
      "Training loss: 104.74\n",
      "Validation evalutaion metric: 0.61\n",
      "Step [605000/1833957]\n",
      "Training evaluation metric: 0.72\n",
      "Training loss: 122.07\n",
      "Step [610000/1833957]\n",
      "Training evaluation metric: 0.52\n",
      "Training loss: 103.14\n",
      "Step [615000/1833957]\n",
      "Training evaluation metric: 0.62\n",
      "Training loss: 103.64\n",
      "Step [620000/1833957]\n",
      "Training evaluation metric: 0.68\n",
      "Training loss: 111.47\n",
      "Step [625000/1833957]\n",
      "Training evaluation metric: 0.66\n",
      "Training loss: 103.82\n",
      "Step [630000/1833957]\n",
      "Training evaluation metric: 0.60\n",
      "Training loss: 112.45\n",
      "Step [635000/1833957]\n",
      "Training evaluation metric: 0.64\n",
      "Training loss: 109.02\n",
      "Step [640000/1833957]\n",
      "Training evaluation metric: 0.54\n",
      "Training loss: 100.55\n",
      "Step [645000/1833957]\n",
      "Training evaluation metric: 0.60\n",
      "Training loss: 99.88\n",
      "Step [650000/1833957]\n",
      "Training evaluation metric: 0.42\n",
      "Training loss: 96.14\n",
      "Validation evalutaion metric: 0.61\n",
      "Step [655000/1833957]\n",
      "Training evaluation metric: 0.62\n",
      "Training loss: 126.16\n",
      "Step [660000/1833957]\n",
      "Training evaluation metric: 0.54\n",
      "Training loss: 96.05\n",
      "Step [665000/1833957]\n",
      "Training evaluation metric: 0.60\n",
      "Training loss: 103.56\n",
      "Step [670000/1833957]\n",
      "Training evaluation metric: 0.62\n",
      "Training loss: 104.94\n",
      "Step [675000/1833957]\n",
      "Training evaluation metric: 0.58\n",
      "Training loss: 100.06\n",
      "Step [680000/1833957]\n",
      "Training evaluation metric: 0.60\n",
      "Training loss: 96.72\n",
      "Step [685000/1833957]\n",
      "Training evaluation metric: 0.62\n",
      "Training loss: 113.62\n",
      "Step [690000/1833957]\n",
      "Training evaluation metric: 0.56\n",
      "Training loss: 105.78\n",
      "Step [695000/1833957]\n",
      "Training evaluation metric: 0.58\n",
      "Training loss: 97.08\n",
      "Step [700000/1833957]\n",
      "Training evaluation metric: 0.60\n",
      "Training loss: 129.79\n",
      "Validation evalutaion metric: 0.60\n",
      "Step [705000/1833957]\n",
      "Training evaluation metric: 0.74\n",
      "Training loss: 115.31\n",
      "Step [710000/1833957]\n",
      "Training evaluation metric: 0.68\n",
      "Training loss: 98.09\n",
      "Step [715000/1833957]\n",
      "Training evaluation metric: 0.62\n",
      "Training loss: 107.20\n",
      "Step [720000/1833957]\n",
      "Training evaluation metric: 0.62\n",
      "Training loss: 99.67\n",
      "Step [725000/1833957]\n",
      "Training evaluation metric: 0.78\n",
      "Training loss: 126.65\n",
      "Step [730000/1833957]\n",
      "Training evaluation metric: 0.42\n",
      "Training loss: 65.82\n",
      "Step [735000/1833957]\n",
      "Training evaluation metric: 0.48\n",
      "Training loss: 77.50\n",
      "Step [740000/1833957]\n",
      "Training evaluation metric: 0.50\n",
      "Training loss: 89.55\n",
      "Step [745000/1833957]\n",
      "Training evaluation metric: 0.54\n",
      "Training loss: 76.46\n",
      "Step [750000/1833957]\n",
      "Training evaluation metric: 0.76\n",
      "Training loss: 110.23\n",
      "Validation evalutaion metric: 0.61\n",
      "Step [755000/1833957]\n",
      "Training evaluation metric: 0.60\n",
      "Training loss: 104.46\n",
      "Step [760000/1833957]\n",
      "Training evaluation metric: 0.58\n",
      "Training loss: 97.53\n",
      "Step [765000/1833957]\n",
      "Training evaluation metric: 0.60\n",
      "Training loss: 83.32\n",
      "Step [770000/1833957]\n",
      "Training evaluation metric: 0.70\n",
      "Training loss: 119.95\n",
      "Step [775000/1833957]\n",
      "Training evaluation metric: 0.62\n",
      "Training loss: 87.59\n",
      "Step [780000/1833957]\n",
      "Training evaluation metric: 0.66\n",
      "Training loss: 104.48\n",
      "Step [785000/1833957]\n",
      "Training evaluation metric: 0.70\n",
      "Training loss: 111.51\n",
      "Step [790000/1833957]\n",
      "Training evaluation metric: 0.62\n",
      "Training loss: 95.61\n",
      "Step [795000/1833957]\n",
      "Training evaluation metric: 0.58\n",
      "Training loss: 97.07\n",
      "Step [800000/1833957]\n",
      "Training evaluation metric: 0.60\n",
      "Training loss: 113.43\n",
      "Validation evalutaion metric: 0.61\n",
      "Step [805000/1833957]\n",
      "Training evaluation metric: 0.54\n",
      "Training loss: 118.29\n",
      "Step [810000/1833957]\n",
      "Training evaluation metric: 0.54\n",
      "Training loss: 92.77\n",
      "Step [815000/1833957]\n",
      "Training evaluation metric: 0.46\n",
      "Training loss: 99.00\n",
      "Step [820000/1833957]\n",
      "Training evaluation metric: 0.54\n",
      "Training loss: 80.00\n",
      "Step [825000/1833957]\n",
      "Training evaluation metric: 0.70\n",
      "Training loss: 119.41\n",
      "Step [830000/1833957]\n",
      "Training evaluation metric: 0.60\n",
      "Training loss: 113.17\n",
      "Step [835000/1833957]\n",
      "Training evaluation metric: 0.54\n",
      "Training loss: 100.31\n",
      "Step [840000/1833957]\n",
      "Training evaluation metric: 0.66\n",
      "Training loss: 109.43\n",
      "Step [845000/1833957]\n",
      "Training evaluation metric: 0.64\n",
      "Training loss: 113.00\n",
      "Step [850000/1833957]\n",
      "Training evaluation metric: 0.70\n",
      "Training loss: 124.45\n",
      "Validation evalutaion metric: 0.61\n",
      "Step [855000/1833957]\n",
      "Training evaluation metric: 0.62\n",
      "Training loss: 107.43\n",
      "Step [860000/1833957]\n",
      "Training evaluation metric: 0.44\n",
      "Training loss: 57.29\n",
      "Step [865000/1833957]\n",
      "Training evaluation metric: 0.50\n",
      "Training loss: 86.21\n",
      "Step [870000/1833957]\n",
      "Training evaluation metric: 0.52\n",
      "Training loss: 96.28\n",
      "Step [875000/1833957]\n",
      "Training evaluation metric: 0.48\n",
      "Training loss: 106.56\n",
      "Step [880000/1833957]\n",
      "Training evaluation metric: 0.62\n",
      "Training loss: 87.38\n",
      "Step [885000/1833957]\n",
      "Training evaluation metric: 0.68\n",
      "Training loss: 120.20\n",
      "Step [890000/1833957]\n",
      "Training evaluation metric: 0.58\n",
      "Training loss: 105.06\n",
      "Step [895000/1833957]\n",
      "Training evaluation metric: 0.68\n",
      "Training loss: 119.55\n",
      "Step [900000/1833957]\n",
      "Training evaluation metric: 0.58\n",
      "Training loss: 88.46\n",
      "Validation evalutaion metric: 0.61\n",
      "Step [905000/1833957]\n",
      "Training evaluation metric: 0.56\n",
      "Training loss: 92.72\n",
      "Step [910000/1833957]\n",
      "Training evaluation metric: 0.66\n",
      "Training loss: 87.06\n",
      "Step [915000/1833957]\n",
      "Training evaluation metric: 0.60\n",
      "Training loss: 91.76\n",
      "Step [920000/1833957]\n",
      "Training evaluation metric: 0.58\n",
      "Training loss: 92.04\n",
      "Step [925000/1833957]\n",
      "Training evaluation metric: 0.62\n",
      "Training loss: 104.40\n",
      "Step [930000/1833957]\n",
      "Training evaluation metric: 0.64\n",
      "Training loss: 112.72\n",
      "Step [935000/1833957]\n",
      "Training evaluation metric: 0.62\n",
      "Training loss: 108.02\n",
      "Step [940000/1833957]\n",
      "Training evaluation metric: 0.64\n",
      "Training loss: 108.74\n",
      "Step [945000/1833957]\n",
      "Training evaluation metric: 0.62\n",
      "Training loss: 111.86\n",
      "Step [950000/1833957]\n",
      "Training evaluation metric: 0.66\n",
      "Training loss: 111.58\n",
      "Validation evalutaion metric: 0.61\n",
      "Step [955000/1833957]\n",
      "Training evaluation metric: 0.74\n",
      "Training loss: 139.59\n",
      "Step [960000/1833957]\n",
      "Training evaluation metric: 0.58\n",
      "Training loss: 96.70\n",
      "Step [965000/1833957]\n",
      "Training evaluation metric: 0.66\n",
      "Training loss: 128.96\n",
      "Step [970000/1833957]\n",
      "Training evaluation metric: 0.62\n",
      "Training loss: 108.63\n",
      "Step [975000/1833957]\n",
      "Training evaluation metric: 0.50\n",
      "Training loss: 76.36\n",
      "Step [980000/1833957]\n",
      "Training evaluation metric: 0.50\n",
      "Training loss: 90.25\n",
      "Step [985000/1833957]\n",
      "Training evaluation metric: 0.64\n",
      "Training loss: 112.97\n",
      "Step [990000/1833957]\n",
      "Training evaluation metric: 0.66\n",
      "Training loss: 95.99\n",
      "Step [995000/1833957]\n",
      "Training evaluation metric: 0.66\n",
      "Training loss: 108.01\n",
      "Step [1000000/1833957]\n",
      "Training evaluation metric: 0.54\n",
      "Training loss: 83.23\n",
      "Validation evalutaion metric: 0.61\n",
      "Step [1005000/1833957]\n",
      "Training evaluation metric: 0.68\n",
      "Training loss: 107.32\n",
      "Step [1010000/1833957]\n",
      "Training evaluation metric: 0.68\n",
      "Training loss: 103.34\n",
      "Step [1015000/1833957]\n",
      "Training evaluation metric: 0.64\n",
      "Training loss: 92.82\n",
      "Step [1020000/1833957]\n",
      "Training evaluation metric: 0.68\n",
      "Training loss: 111.77\n",
      "Step [1025000/1833957]\n",
      "Training evaluation metric: 0.56\n",
      "Training loss: 100.95\n",
      "Step [1030000/1833957]\n",
      "Training evaluation metric: 0.68\n",
      "Training loss: 106.67\n",
      "Step [1035000/1833957]\n",
      "Training evaluation metric: 0.60\n",
      "Training loss: 98.44\n",
      "Step [1040000/1833957]\n",
      "Training evaluation metric: 0.62\n",
      "Training loss: 103.27\n",
      "Step [1045000/1833957]\n",
      "Training evaluation metric: 0.46\n",
      "Training loss: 86.52\n",
      "Step [1050000/1833957]\n",
      "Training evaluation metric: 0.60\n",
      "Training loss: 104.56\n",
      "Validation evalutaion metric: 0.61\n",
      "Step [1055000/1833957]\n",
      "Training evaluation metric: 0.60\n",
      "Training loss: 103.95\n",
      "Step [1060000/1833957]\n",
      "Training evaluation metric: 0.64\n",
      "Training loss: 123.68\n",
      "Step [1065000/1833957]\n",
      "Training evaluation metric: 0.60\n",
      "Training loss: 95.43\n",
      "Step [1070000/1833957]\n",
      "Training evaluation metric: 0.66\n",
      "Training loss: 132.15\n",
      "Step [1075000/1833957]\n",
      "Training evaluation metric: 0.58\n",
      "Training loss: 95.69\n",
      "Step [1080000/1833957]\n",
      "Training evaluation metric: 0.54\n",
      "Training loss: 97.37\n",
      "Step [1085000/1833957]\n",
      "Training evaluation metric: 0.54\n",
      "Training loss: 113.95\n",
      "Step [1090000/1833957]\n",
      "Training evaluation metric: 0.56\n",
      "Training loss: 79.09\n",
      "Step [1095000/1833957]\n",
      "Training evaluation metric: 0.62\n",
      "Training loss: 112.47\n",
      "Step [1100000/1833957]\n",
      "Training evaluation metric: 0.54\n",
      "Training loss: 92.78\n",
      "Validation evalutaion metric: 0.61\n",
      "Step [1105000/1833957]\n",
      "Training evaluation metric: 0.56\n",
      "Training loss: 93.21\n",
      "Step [1110000/1833957]\n",
      "Training evaluation metric: 0.58\n",
      "Training loss: 86.99\n",
      "Step [1115000/1833957]\n",
      "Training evaluation metric: 0.62\n",
      "Training loss: 120.24\n",
      "Step [1120000/1833957]\n",
      "Training evaluation metric: 0.52\n",
      "Training loss: 99.22\n",
      "Step [1125000/1833957]\n",
      "Training evaluation metric: 0.58\n",
      "Training loss: 116.79\n",
      "Step [1130000/1833957]\n",
      "Training evaluation metric: 0.58\n",
      "Training loss: 92.10\n",
      "Step [1135000/1833957]\n",
      "Training evaluation metric: 0.64\n",
      "Training loss: 111.01\n",
      "Step [1140000/1833957]\n",
      "Training evaluation metric: 0.56\n",
      "Training loss: 92.50\n",
      "Step [1145000/1833957]\n",
      "Training evaluation metric: 0.58\n",
      "Training loss: 120.89\n",
      "Step [1150000/1833957]\n",
      "Training evaluation metric: 0.68\n",
      "Training loss: 122.76\n",
      "Validation evalutaion metric: 0.61\n",
      "Step [1155000/1833957]\n",
      "Training evaluation metric: 0.64\n",
      "Training loss: 95.97\n",
      "Step [1160000/1833957]\n",
      "Training evaluation metric: 0.52\n",
      "Training loss: 92.71\n",
      "Step [1165000/1833957]\n",
      "Training evaluation metric: 0.70\n",
      "Training loss: 136.75\n",
      "Step [1170000/1833957]\n",
      "Training evaluation metric: 0.68\n",
      "Training loss: 115.97\n",
      "Step [1175000/1833957]\n",
      "Training evaluation metric: 0.74\n",
      "Training loss: 106.30\n",
      "Step [1180000/1833957]\n",
      "Training evaluation metric: 0.58\n",
      "Training loss: 100.11\n",
      "Step [1185000/1833957]\n",
      "Training evaluation metric: 0.58\n",
      "Training loss: 108.85\n",
      "Step [1190000/1833957]\n",
      "Training evaluation metric: 0.66\n",
      "Training loss: 108.80\n",
      "Step [1195000/1833957]\n",
      "Training evaluation metric: 0.50\n",
      "Training loss: 108.94\n",
      "Step [1200000/1833957]\n",
      "Training evaluation metric: 0.56\n",
      "Training loss: 91.41\n",
      "Validation evalutaion metric: 0.61\n",
      "Step [1205000/1833957]\n",
      "Training evaluation metric: 0.64\n",
      "Training loss: 104.15\n",
      "Step [1210000/1833957]\n",
      "Training evaluation metric: 0.60\n",
      "Training loss: 95.98\n",
      "Step [1215000/1833957]\n",
      "Training evaluation metric: 0.58\n",
      "Training loss: 105.07\n",
      "Step [1220000/1833957]\n",
      "Training evaluation metric: 0.48\n",
      "Training loss: 69.17\n",
      "Step [1225000/1833957]\n",
      "Training evaluation metric: 0.58\n",
      "Training loss: 75.39\n",
      "Step [1230000/1833957]\n",
      "Training evaluation metric: 0.60\n",
      "Training loss: 104.94\n",
      "Step [1235000/1833957]\n",
      "Training evaluation metric: 0.60\n",
      "Training loss: 91.83\n",
      "Step [1240000/1833957]\n",
      "Training evaluation metric: 0.58\n",
      "Training loss: 95.74\n",
      "Step [1245000/1833957]\n",
      "Training evaluation metric: 0.50\n",
      "Training loss: 84.78\n",
      "Step [1250000/1833957]\n",
      "Training evaluation metric: 0.56\n",
      "Training loss: 106.31\n",
      "Validation evalutaion metric: 0.61\n",
      "Step [1255000/1833957]\n",
      "Training evaluation metric: 0.60\n",
      "Training loss: 108.28\n",
      "Step [1260000/1833957]\n",
      "Training evaluation metric: 0.62\n",
      "Training loss: 108.13\n",
      "Step [1265000/1833957]\n",
      "Training evaluation metric: 0.50\n",
      "Training loss: 94.38\n",
      "Step [1270000/1833957]\n",
      "Training evaluation metric: 0.60\n",
      "Training loss: 109.42\n",
      "Step [1275000/1833957]\n",
      "Training evaluation metric: 0.58\n",
      "Training loss: 87.57\n",
      "Step [1280000/1833957]\n",
      "Training evaluation metric: 0.72\n",
      "Training loss: 127.99\n",
      "Step [1285000/1833957]\n",
      "Training evaluation metric: 0.66\n",
      "Training loss: 116.21\n",
      "Step [1290000/1833957]\n",
      "Training evaluation metric: 0.54\n",
      "Training loss: 83.07\n",
      "Step [1295000/1833957]\n",
      "Training evaluation metric: 0.60\n",
      "Training loss: 71.81\n",
      "Step [1300000/1833957]\n",
      "Training evaluation metric: 0.58\n",
      "Training loss: 92.76\n",
      "Validation evalutaion metric: 0.61\n",
      "Step [1305000/1833957]\n",
      "Training evaluation metric: 0.78\n",
      "Training loss: 117.48\n",
      "Step [1310000/1833957]\n",
      "Training evaluation metric: 0.66\n",
      "Training loss: 116.95\n",
      "Step [1315000/1833957]\n",
      "Training evaluation metric: 0.58\n",
      "Training loss: 67.82\n",
      "Step [1320000/1833957]\n",
      "Training evaluation metric: 0.52\n",
      "Training loss: 88.38\n",
      "Step [1325000/1833957]\n",
      "Training evaluation metric: 0.58\n",
      "Training loss: 96.05\n",
      "Step [1330000/1833957]\n",
      "Training evaluation metric: 0.62\n",
      "Training loss: 105.72\n",
      "Step [1335000/1833957]\n",
      "Training evaluation metric: 0.70\n",
      "Training loss: 112.46\n",
      "Step [1340000/1833957]\n",
      "Training evaluation metric: 0.56\n",
      "Training loss: 76.19\n",
      "Step [1345000/1833957]\n",
      "Training evaluation metric: 0.68\n",
      "Training loss: 117.20\n",
      "Step [1350000/1833957]\n",
      "Training evaluation metric: 0.54\n",
      "Training loss: 97.48\n",
      "Validation evalutaion metric: 0.61\n",
      "Step [1355000/1833957]\n",
      "Training evaluation metric: 0.66\n",
      "Training loss: 108.19\n",
      "Step [1360000/1833957]\n",
      "Training evaluation metric: 0.62\n",
      "Training loss: 87.99\n",
      "Step [1365000/1833957]\n",
      "Training evaluation metric: 0.64\n",
      "Training loss: 108.63\n",
      "Step [1370000/1833957]\n",
      "Training evaluation metric: 0.62\n",
      "Training loss: 108.87\n",
      "Step [1375000/1833957]\n",
      "Training evaluation metric: 0.54\n",
      "Training loss: 84.16\n",
      "Step [1380000/1833957]\n",
      "Training evaluation metric: 0.48\n",
      "Training loss: 81.40\n",
      "Step [1385000/1833957]\n",
      "Training evaluation metric: 0.54\n",
      "Training loss: 76.25\n",
      "Step [1390000/1833957]\n",
      "Training evaluation metric: 0.64\n",
      "Training loss: 94.74\n",
      "Step [1395000/1833957]\n",
      "Training evaluation metric: 0.78\n",
      "Training loss: 135.14\n",
      "Step [1400000/1833957]\n",
      "Training evaluation metric: 0.58\n",
      "Training loss: 88.06\n",
      "Validation evalutaion metric: 0.61\n",
      "Step [1405000/1833957]\n",
      "Training evaluation metric: 0.54\n",
      "Training loss: 95.55\n",
      "Step [1410000/1833957]\n",
      "Training evaluation metric: 0.66\n",
      "Training loss: 112.32\n",
      "Step [1415000/1833957]\n",
      "Training evaluation metric: 0.66\n",
      "Training loss: 91.56\n",
      "Step [1420000/1833957]\n",
      "Training evaluation metric: 0.66\n",
      "Training loss: 98.13\n",
      "Step [1425000/1833957]\n",
      "Training evaluation metric: 0.52\n",
      "Training loss: 105.49\n",
      "Step [1430000/1833957]\n",
      "Training evaluation metric: 0.72\n",
      "Training loss: 127.35\n",
      "Step [1435000/1833957]\n",
      "Training evaluation metric: 0.64\n",
      "Training loss: 111.44\n",
      "Step [1440000/1833957]\n",
      "Training evaluation metric: 0.58\n",
      "Training loss: 96.18\n",
      "Step [1445000/1833957]\n",
      "Training evaluation metric: 0.62\n",
      "Training loss: 112.35\n",
      "Step [1450000/1833957]\n",
      "Training evaluation metric: 0.60\n",
      "Training loss: 108.64\n",
      "Validation evalutaion metric: 0.61\n",
      "Step [1455000/1833957]\n",
      "Training evaluation metric: 0.72\n",
      "Training loss: 114.84\n",
      "Step [1460000/1833957]\n",
      "Training evaluation metric: 0.56\n",
      "Training loss: 87.93\n",
      "Step [1465000/1833957]\n",
      "Training evaluation metric: 0.60\n",
      "Training loss: 114.41\n",
      "Step [1470000/1833957]\n",
      "Training evaluation metric: 0.58\n",
      "Training loss: 100.25\n",
      "Step [1475000/1833957]\n",
      "Training evaluation metric: 0.50\n",
      "Training loss: 102.09\n",
      "Step [1480000/1833957]\n",
      "Training evaluation metric: 0.78\n",
      "Training loss: 128.02\n",
      "Step [1485000/1833957]\n",
      "Training evaluation metric: 0.62\n",
      "Training loss: 99.10\n",
      "Step [1490000/1833957]\n",
      "Training evaluation metric: 0.66\n",
      "Training loss: 109.24\n",
      "Step [1495000/1833957]\n",
      "Training evaluation metric: 0.58\n",
      "Training loss: 96.64\n",
      "Step [1500000/1833957]\n",
      "Training evaluation metric: 0.58\n",
      "Training loss: 108.81\n",
      "Validation evalutaion metric: 0.61\n",
      "Step [1505000/1833957]\n",
      "Training evaluation metric: 0.58\n",
      "Training loss: 84.83\n",
      "Step [1510000/1833957]\n",
      "Training evaluation metric: 0.56\n",
      "Training loss: 96.76\n",
      "Step [1515000/1833957]\n",
      "Training evaluation metric: 0.66\n",
      "Training loss: 128.46\n",
      "Step [1520000/1833957]\n",
      "Training evaluation metric: 0.64\n",
      "Training loss: 107.78\n",
      "Step [1525000/1833957]\n",
      "Training evaluation metric: 0.50\n",
      "Training loss: 113.95\n",
      "Step [1530000/1833957]\n",
      "Training evaluation metric: 0.60\n",
      "Training loss: 117.60\n",
      "Step [1535000/1833957]\n",
      "Training evaluation metric: 0.60\n",
      "Training loss: 89.14\n",
      "Step [1540000/1833957]\n",
      "Training evaluation metric: 0.62\n",
      "Training loss: 96.43\n",
      "Step [1545000/1833957]\n",
      "Training evaluation metric: 0.60\n",
      "Training loss: 108.05\n",
      "Step [1550000/1833957]\n",
      "Training evaluation metric: 0.62\n",
      "Training loss: 108.67\n",
      "Validation evalutaion metric: 0.60\n",
      "Step [1555000/1833957]\n",
      "Training evaluation metric: 0.50\n",
      "Training loss: 105.96\n",
      "Step [1560000/1833957]\n",
      "Training evaluation metric: 0.62\n",
      "Training loss: 104.68\n",
      "Step [1565000/1833957]\n",
      "Training evaluation metric: 0.54\n",
      "Training loss: 96.68\n",
      "Step [1570000/1833957]\n",
      "Training evaluation metric: 0.62\n",
      "Training loss: 100.58\n",
      "Step [1575000/1833957]\n",
      "Training evaluation metric: 0.62\n",
      "Training loss: 112.14\n",
      "Step [1580000/1833957]\n",
      "Training evaluation metric: 0.72\n",
      "Training loss: 118.27\n",
      "Step [1585000/1833957]\n",
      "Training evaluation metric: 0.54\n",
      "Training loss: 96.58\n",
      "Step [1590000/1833957]\n",
      "Training evaluation metric: 0.66\n",
      "Training loss: 125.16\n",
      "Step [1595000/1833957]\n",
      "Training evaluation metric: 0.58\n",
      "Training loss: 96.55\n",
      "Step [1600000/1833957]\n",
      "Training evaluation metric: 0.60\n",
      "Training loss: 87.97\n",
      "Validation evalutaion metric: 0.60\n",
      "Step [1605000/1833957]\n",
      "Training evaluation metric: 0.70\n",
      "Training loss: 123.28\n",
      "Step [1610000/1833957]\n",
      "Training evaluation metric: 0.60\n",
      "Training loss: 107.94\n",
      "Step [1615000/1833957]\n",
      "Training evaluation metric: 0.60\n",
      "Training loss: 108.10\n",
      "Step [1620000/1833957]\n",
      "Training evaluation metric: 0.78\n",
      "Training loss: 117.18\n",
      "Step [1625000/1833957]\n",
      "Training evaluation metric: 0.72\n",
      "Training loss: 131.54\n",
      "Step [1630000/1833957]\n",
      "Training evaluation metric: 0.66\n",
      "Training loss: 124.16\n",
      "Step [1635000/1833957]\n",
      "Training evaluation metric: 0.52\n",
      "Training loss: 99.97\n",
      "Step [1640000/1833957]\n",
      "Training evaluation metric: 0.60\n",
      "Training loss: 95.33\n",
      "Step [1645000/1833957]\n",
      "Training evaluation metric: 0.40\n",
      "Training loss: 74.09\n",
      "Step [1650000/1833957]\n",
      "Training evaluation metric: 0.60\n",
      "Training loss: 83.45\n",
      "Validation evalutaion metric: 0.61\n",
      "Step [1655000/1833957]\n",
      "Training evaluation metric: 0.62\n",
      "Training loss: 120.63\n",
      "Step [1660000/1833957]\n",
      "Training evaluation metric: 0.52\n",
      "Training loss: 96.53\n",
      "Step [1665000/1833957]\n",
      "Training evaluation metric: 0.60\n",
      "Training loss: 121.60\n",
      "Step [1670000/1833957]\n",
      "Training evaluation metric: 0.58\n",
      "Training loss: 101.69\n",
      "Step [1675000/1833957]\n",
      "Training evaluation metric: 0.62\n",
      "Training loss: 111.41\n",
      "Step [1680000/1833957]\n",
      "Training evaluation metric: 0.62\n",
      "Training loss: 107.40\n",
      "Step [1685000/1833957]\n",
      "Training evaluation metric: 0.64\n",
      "Training loss: 112.02\n",
      "Step [1690000/1833957]\n",
      "Training evaluation metric: 0.60\n",
      "Training loss: 83.77\n",
      "Step [1695000/1833957]\n",
      "Training evaluation metric: 0.60\n",
      "Training loss: 108.64\n",
      "Step [1700000/1833957]\n",
      "Training evaluation metric: 0.70\n",
      "Training loss: 110.38\n",
      "Validation evalutaion metric: 0.61\n",
      "Step [1705000/1833957]\n",
      "Training evaluation metric: 0.58\n",
      "Training loss: 93.74\n",
      "Step [1710000/1833957]\n",
      "Training evaluation metric: 0.62\n",
      "Training loss: 120.09\n",
      "Step [1715000/1833957]\n",
      "Training evaluation metric: 0.62\n",
      "Training loss: 104.15\n",
      "Step [1720000/1833957]\n",
      "Training evaluation metric: 0.68\n",
      "Training loss: 119.81\n",
      "Step [1725000/1833957]\n",
      "Training evaluation metric: 0.60\n",
      "Training loss: 104.86\n",
      "Step [1730000/1833957]\n",
      "Training evaluation metric: 0.62\n",
      "Training loss: 88.08\n",
      "Step [1735000/1833957]\n",
      "Training evaluation metric: 0.66\n",
      "Training loss: 99.10\n",
      "Step [1740000/1833957]\n",
      "Training evaluation metric: 0.58\n",
      "Training loss: 75.35\n",
      "Step [1745000/1833957]\n",
      "Training evaluation metric: 0.56\n",
      "Training loss: 101.81\n",
      "Step [1750000/1833957]\n",
      "Training evaluation metric: 0.70\n",
      "Training loss: 120.19\n",
      "Validation evalutaion metric: 0.60\n",
      "Step [1755000/1833957]\n",
      "Training evaluation metric: 0.46\n",
      "Training loss: 84.38\n",
      "Step [1760000/1833957]\n",
      "Training evaluation metric: 0.62\n",
      "Training loss: 112.78\n",
      "Step [1765000/1833957]\n",
      "Training evaluation metric: 0.50\n",
      "Training loss: 92.33\n",
      "Step [1770000/1833957]\n",
      "Training evaluation metric: 0.60\n",
      "Training loss: 108.34\n",
      "Step [1775000/1833957]\n",
      "Training evaluation metric: 0.74\n",
      "Training loss: 114.31\n",
      "Step [1780000/1833957]\n",
      "Training evaluation metric: 0.54\n",
      "Training loss: 108.71\n",
      "Step [1785000/1833957]\n",
      "Training evaluation metric: 0.72\n",
      "Training loss: 123.55\n",
      "Step [1790000/1833957]\n",
      "Training evaluation metric: 0.50\n",
      "Training loss: 90.39\n",
      "Step [1795000/1833957]\n",
      "Training evaluation metric: 0.72\n",
      "Training loss: 140.73\n",
      "Step [1800000/1833957]\n",
      "Training evaluation metric: 0.62\n",
      "Training loss: 103.38\n",
      "Validation evalutaion metric: 0.61\n",
      "Step [1805000/1833957]\n",
      "Training evaluation metric: 0.58\n",
      "Training loss: 113.43\n",
      "Step [1810000/1833957]\n",
      "Training evaluation metric: 0.56\n",
      "Training loss: 88.34\n",
      "Step [1815000/1833957]\n",
      "Training evaluation metric: 0.60\n",
      "Training loss: 107.15\n",
      "Step [1820000/1833957]\n",
      "Training evaluation metric: 0.64\n",
      "Training loss: 112.62\n",
      "Step [1825000/1833957]\n",
      "Training evaluation metric: 0.50\n",
      "Training loss: 85.20\n",
      "Step [1830000/1833957]\n",
      "Training evaluation metric: 0.60\n",
      "Training loss: 104.81\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_and_validation_loop(nn_model, train_dataloader, val_dataloader, loss_fn, optimizer, torch_accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 620.119385  [    0/1833957]\n",
      "loss: 630.280884  [20000/1833957]\n",
      "loss: 524.894653  [40000/1833957]\n",
      "loss: 593.669250  [60000/1833957]\n",
      "loss: 571.914673  [80000/1833957]\n",
      "loss: 588.110535  [100000/1833957]\n",
      "loss: 513.986816  [120000/1833957]\n",
      "loss: 603.819824  [140000/1833957]\n",
      "loss: 599.019836  [160000/1833957]\n",
      "loss: 583.026550  [180000/1833957]\n",
      "loss: 551.539429  [200000/1833957]\n",
      "loss: 593.872192  [220000/1833957]\n",
      "loss: 588.486267  [240000/1833957]\n",
      "loss: 514.005493  [260000/1833957]\n",
      "loss: 576.875549  [280000/1833957]\n",
      "loss: 540.469299  [300000/1833957]\n",
      "loss: 599.091064  [320000/1833957]\n",
      "loss: 620.141846  [340000/1833957]\n",
      "loss: 556.705627  [360000/1833957]\n",
      "loss: 545.881714  [380000/1833957]\n",
      "loss: 582.870239  [400000/1833957]\n",
      "loss: 593.069641  [420000/1833957]\n",
      "loss: 540.596802  [440000/1833957]\n",
      "loss: 567.101624  [460000/1833957]\n",
      "loss: 577.834839  [480000/1833957]\n",
      "loss: 556.476929  [500000/1833957]\n",
      "loss: 620.127808  [520000/1833957]\n",
      "loss: 534.875916  [540000/1833957]\n",
      "loss: 582.892395  [560000/1833957]\n",
      "loss: 529.658203  [580000/1833957]\n",
      "loss: 566.632751  [600000/1833957]\n",
      "loss: 551.198242  [620000/1833957]\n",
      "loss: 582.609253  [640000/1833957]\n",
      "loss: 625.017639  [660000/1833957]\n",
      "loss: 545.900818  [680000/1833957]\n",
      "loss: 540.899780  [700000/1833957]\n",
      "loss: 519.567017  [720000/1833957]\n",
      "loss: 556.491821  [740000/1833957]\n",
      "loss: 592.896118  [760000/1833957]\n",
      "loss: 566.612183  [780000/1833957]\n",
      "loss: 556.594299  [800000/1833957]\n",
      "loss: 593.566956  [820000/1833957]\n",
      "loss: 624.914795  [840000/1833957]\n",
      "loss: 534.973511  [860000/1833957]\n",
      "loss: 572.809875  [880000/1833957]\n",
      "loss: 587.933533  [900000/1833957]\n",
      "loss: 561.708984  [920000/1833957]\n",
      "loss: 524.600464  [940000/1833957]\n",
      "loss: 588.584656  [960000/1833957]\n",
      "loss: 550.916870  [980000/1833957]\n",
      "loss: 535.069702  [1000000/1833957]\n",
      "loss: 508.627625  [1020000/1833957]\n",
      "loss: 593.423828  [1040000/1833957]\n",
      "loss: 545.784607  [1060000/1833957]\n",
      "loss: 641.344788  [1080000/1833957]\n",
      "loss: 572.452271  [1100000/1833957]\n",
      "loss: 561.652527  [1120000/1833957]\n",
      "loss: 582.815979  [1140000/1833957]\n",
      "loss: 587.881653  [1160000/1833957]\n",
      "loss: 561.998596  [1180000/1833957]\n",
      "loss: 578.157532  [1200000/1833957]\n",
      "loss: 572.095642  [1220000/1833957]\n",
      "loss: 578.029480  [1240000/1833957]\n",
      "loss: 567.615234  [1260000/1833957]\n",
      "loss: 540.580383  [1280000/1833957]\n",
      "loss: 662.029785  [1300000/1833957]\n",
      "loss: 561.419006  [1320000/1833957]\n",
      "loss: 592.997864  [1340000/1833957]\n",
      "loss: 635.751221  [1360000/1833957]\n",
      "loss: 582.934204  [1380000/1833957]\n",
      "loss: 545.694214  [1400000/1833957]\n",
      "loss: 598.822510  [1420000/1833957]\n",
      "loss: 545.709961  [1440000/1833957]\n",
      "loss: 593.758240  [1460000/1833957]\n",
      "loss: 641.103394  [1480000/1833957]\n",
      "loss: 598.876099  [1500000/1833957]\n",
      "loss: 546.160278  [1520000/1833957]\n",
      "loss: 556.753174  [1540000/1833957]\n",
      "loss: 529.429199  [1560000/1833957]\n",
      "loss: 588.764648  [1580000/1833957]\n",
      "loss: 577.314209  [1600000/1833957]\n",
      "loss: 583.281555  [1620000/1833957]\n",
      "loss: 540.341492  [1640000/1833957]\n",
      "loss: 593.516785  [1660000/1833957]\n",
      "loss: 530.053711  [1680000/1833957]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\projects\\oracle\\research\\data\\test_api.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/projects/oracle/research/data/test_api.ipynb#Y102sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/projects/oracle/research/data/test_api.ipynb#Y102sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-------------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/projects/oracle/research/data/test_api.ipynb#Y102sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     train_loop(train_dataloader, nn_model, loss_fn, optimizer)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/projects/oracle/research/data/test_api.ipynb#Y102sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     test_loop(val_dataloader, nn_model, loss_fn)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/projects/oracle/research/data/test_api.ipynb#Y102sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDone!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32md:\\projects\\oracle\\research\\data\\test_api.ipynb Cell 19\u001b[0m in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/projects/oracle/research/data/test_api.ipynb#Y102sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch, (X, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/projects/oracle/research/data/test_api.ipynb#Y102sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m# Compute prediction and loss\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/projects/oracle/research/data/test_api.ipynb#Y102sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     pred \u001b[39m=\u001b[39m model(X)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/projects/oracle/research/data/test_api.ipynb#Y102sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(pred, y)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/projects/oracle/research/data/test_api.ipynb#Y102sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m# Backpropagation\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/projects/oracle/research/data/test_api.ipynb#Y102sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\dagon\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\dagon\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1164\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m-> 1164\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m   1165\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[0;32m   1166\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\dagon\\lib\\site-packages\\torch\\nn\\functional.py:3014\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3012\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3013\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3014\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, nn_model, loss_fn, optimizer)\n",
    "    test_loop(val_dataloader, nn_model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch, (X, y)  in enumerate(train_dataloader):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
       "        0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "        1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0.,\n",
       "        0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1.,\n",
       "        1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0.,\n",
       "        0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
       "        1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
       "        0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
       "        0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1.,\n",
       "        1., 1.])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1555, -0.1228, -0.1312, -0.1111, -0.0725, -0.1061, -0.1515, -0.1753,\n",
       "        -0.1232, -0.1279, -0.1214, -0.1590, -0.1516, -0.1602, -0.1703, -0.1225,\n",
       "        -0.1626, -0.1302, -0.1803, -0.1560, -0.1275, -0.1634, -0.1836, -0.1128,\n",
       "        -0.1270, -0.1750, -0.1635, -0.1610, -0.1698, -0.1490, -0.1615, -0.1072,\n",
       "        -0.1124, -0.1418, -0.1231, -0.2072, -0.1214, -0.0889, -0.1241, -0.1790,\n",
       "        -0.1220, -0.1740, -0.1540, -0.1398, -0.1411, -0.1033, -0.1837, -0.0973,\n",
       "        -0.1617, -0.1669, -0.1571, -0.1185, -0.1869, -0.1451, -0.1231, -0.1248,\n",
       "        -0.1488, -0.1134, -0.1098, -0.1451, -0.1569, -0.1361, -0.1857, -0.1319,\n",
       "        -0.0869, -0.1439, -0.1495, -0.1214, -0.1719, -0.1140, -0.1026, -0.1362,\n",
       "        -0.1670, -0.1615, -0.1355, -0.1282, -0.1243, -0.1846, -0.1504, -0.1601,\n",
       "        -0.1006, -0.1227, -0.1516, -0.1607, -0.1232, -0.1433, -0.1498, -0.1041,\n",
       "        -0.1311, -0.1423, -0.1724, -0.1653, -0.1040, -0.1594, -0.1384, -0.1316,\n",
       "        -0.1486, -0.1540, -0.1282, -0.1479, -0.1630, -0.1672, -0.1367, -0.1606,\n",
       "        -0.1343, -0.1241, -0.1379, -0.1472, -0.1184, -0.1530, -0.1294, -0.1356,\n",
       "        -0.1414, -0.1589, -0.1510, -0.0987, -0.1275, -0.1319, -0.1300, -0.1580,\n",
       "        -0.1267, -0.1336, -0.1034, -0.1466, -0.1353, -0.1347, -0.1201, -0.1789,\n",
       "        -0.1429, -0.1493, -0.1527, -0.1117, -0.1678, -0.1738, -0.1525, -0.1561,\n",
       "        -0.1337, -0.1653, -0.1125, -0.1611, -0.1420, -0.1227, -0.1412, -0.1687,\n",
       "        -0.1807, -0.1250, -0.1119, -0.1593, -0.1564, -0.1492, -0.1404, -0.1329,\n",
       "        -0.2060, -0.1931, -0.0871, -0.1763, -0.2279, -0.1211, -0.1681, -0.0904,\n",
       "        -0.1512, -0.1368, -0.1510, -0.1354, -0.1488, -0.1542, -0.0921, -0.1478,\n",
       "        -0.1153, -0.1536, -0.1709, -0.1036, -0.1365, -0.1459, -0.0928, -0.0936,\n",
       "        -0.1126, -0.1537, -0.1345, -0.0991, -0.1294, -0.1116, -0.1852, -0.1005,\n",
       "        -0.1007, -0.1113, -0.1590, -0.2024, -0.0879, -0.1095, -0.1834, -0.1633,\n",
       "        -0.1512, -0.1434, -0.1261, -0.1006, -0.1995, -0.1819, -0.1847, -0.1298],\n",
       "       grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model.forward(X).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Long but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\projects\\oracle\\research\\data\\test_api.ipynb Cell 21\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/projects/oracle/research/data/test_api.ipynb#Y104sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m loss_fn(nn_model\u001b[39m.\u001b[39;49mforward(X), y)\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\dagon\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\dagon\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1164\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m-> 1164\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m   1165\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[0;32m   1166\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\dagon\\lib\\site-packages\\torch\\nn\\functional.py:3014\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3012\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3013\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3014\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected scalar type Long but found Float"
     ]
    }
   ],
   "source": [
    "loss_fn(nn_model.forward(X), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 128])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.8912e-01, -5.7425e-02],\n",
       "        [-1.5475e-01, -1.4809e-01],\n",
       "        [-7.1998e-02, -1.5522e-01],\n",
       "        [-1.0326e-01, -7.7930e-02],\n",
       "        [-1.5057e-01, -2.9486e-02],\n",
       "        [-9.6504e-02,  1.8674e-02],\n",
       "        [-1.1713e-01, -1.1737e-02],\n",
       "        [-7.7100e-02, -1.0176e-01],\n",
       "        [-9.4819e-02,  9.2966e-03],\n",
       "        [-9.7504e-02, -2.8424e-02],\n",
       "        [-1.7037e-01, -8.6623e-02],\n",
       "        [-1.0515e-01, -3.6761e-02],\n",
       "        [-3.4627e-02, -9.0209e-02],\n",
       "        [-5.1570e-02, -5.1420e-02],\n",
       "        [-1.3527e-01, -6.6940e-02],\n",
       "        [-1.2376e-01, -4.7756e-02],\n",
       "        [-1.0255e-01, -5.1782e-02],\n",
       "        [-8.6992e-02, -3.1702e-02],\n",
       "        [-3.5523e-02, -8.6282e-02],\n",
       "        [-8.8603e-02,  1.8601e-02],\n",
       "        [-1.8063e-01, -1.1380e-01],\n",
       "        [-8.4285e-02,  4.9493e-02],\n",
       "        [-1.0022e-01, -3.1401e-02],\n",
       "        [-1.0214e-01, -2.9818e-02],\n",
       "        [-1.0784e-01, -3.4894e-02],\n",
       "        [-1.8008e-01, -9.0804e-02],\n",
       "        [-1.0357e-01, -1.0753e-01],\n",
       "        [-1.2374e-01, -9.2724e-02],\n",
       "        [-8.5113e-02, -1.2209e-01],\n",
       "        [-6.8426e-02, -4.3046e-02],\n",
       "        [-1.0256e-01, -1.0349e-01],\n",
       "        [-1.1244e-01, -9.9832e-02],\n",
       "        [-5.7832e-02, -1.0939e-01],\n",
       "        [-7.5502e-02, -1.0083e-01],\n",
       "        [-6.4902e-02, -5.8572e-02],\n",
       "        [-8.3186e-02, -1.2733e-01],\n",
       "        [-6.5016e-02, -6.8660e-02],\n",
       "        [-5.4470e-02, -8.3165e-02],\n",
       "        [-9.9670e-02, -4.5156e-02],\n",
       "        [-6.2865e-02, -4.7094e-02],\n",
       "        [-1.0488e-01, -6.6897e-02],\n",
       "        [-1.0878e-01, -9.8469e-02],\n",
       "        [-5.5979e-02, -1.2930e-01],\n",
       "        [-1.4547e-01, -4.1105e-02],\n",
       "        [-1.0809e-01, -6.0169e-02],\n",
       "        [-1.5105e-01, -7.1801e-02],\n",
       "        [-5.6554e-02, -3.4355e-02],\n",
       "        [-9.8719e-02, -7.2228e-02],\n",
       "        [-1.3498e-01, -1.0494e-01],\n",
       "        [-1.4696e-01, -5.8534e-02],\n",
       "        [-1.3548e-01, -1.0060e-01],\n",
       "        [-1.2931e-01, -8.7578e-03],\n",
       "        [-8.4660e-02, -9.6528e-02],\n",
       "        [-5.9218e-02, -4.9901e-02],\n",
       "        [-1.3278e-01, -3.4294e-02],\n",
       "        [-1.0832e-01, -1.7932e-01],\n",
       "        [-9.1157e-02, -8.3206e-02],\n",
       "        [-1.3237e-01, -4.7138e-02],\n",
       "        [-1.3264e-01, -8.1753e-02],\n",
       "        [-5.5418e-02, -4.2194e-03],\n",
       "        [-2.1254e-01, -8.8899e-02],\n",
       "        [-3.2997e-02, -5.8872e-02],\n",
       "        [-8.9022e-02, -1.1224e-01],\n",
       "        [-2.2784e-02, -5.5586e-02],\n",
       "        [-1.5049e-01, -2.5471e-02],\n",
       "        [-6.2031e-02, -3.6081e-02],\n",
       "        [-1.0552e-01,  8.4523e-03],\n",
       "        [-7.5778e-02, -9.8497e-02],\n",
       "        [-1.3653e-01,  1.8515e-02],\n",
       "        [-1.2676e-01, -6.8569e-02],\n",
       "        [-1.9307e-01, -6.3862e-02],\n",
       "        [-8.5419e-02, -1.4065e-02],\n",
       "        [-8.6568e-02, -4.3828e-02],\n",
       "        [-6.3933e-02, -6.9382e-02],\n",
       "        [-8.1292e-02, -9.2878e-02],\n",
       "        [-1.3110e-01, -1.5141e-01],\n",
       "        [-5.3437e-02, -5.0698e-02],\n",
       "        [-1.0320e-01, -1.2011e-01],\n",
       "        [-1.3702e-01, -2.7492e-02],\n",
       "        [-9.4685e-02, -1.2180e-01],\n",
       "        [-1.2213e-01, -1.1485e-01],\n",
       "        [-6.7085e-02, -5.9659e-02],\n",
       "        [-1.1071e-01,  5.9072e-03],\n",
       "        [-1.0648e-01, -2.5935e-02],\n",
       "        [-1.9961e-01,  6.8863e-02],\n",
       "        [-9.0828e-02, -6.3918e-02],\n",
       "        [-6.9621e-02, -5.2113e-02],\n",
       "        [-8.2075e-02,  1.1482e-03],\n",
       "        [-3.6251e-02, -5.7511e-03],\n",
       "        [-6.9040e-02, -4.7698e-02],\n",
       "        [-2.0649e-02, -6.5148e-02],\n",
       "        [-1.2056e-01, -2.1895e-02],\n",
       "        [-1.2423e-01, -8.9633e-02],\n",
       "        [-8.2991e-02, -9.3886e-02],\n",
       "        [-7.0858e-02,  3.0958e-02],\n",
       "        [-1.1665e-01, -9.7350e-02],\n",
       "        [-1.3076e-01, -1.1455e-01],\n",
       "        [-8.1052e-02, -7.4901e-02],\n",
       "        [-2.7804e-02, -6.5882e-02],\n",
       "        [-1.8153e-02, -4.6983e-03],\n",
       "        [-7.7843e-02, -1.0295e-01],\n",
       "        [-1.0520e-01, -6.4526e-02],\n",
       "        [-7.4739e-02,  5.2448e-02],\n",
       "        [-9.8058e-02, -6.3704e-02],\n",
       "        [-7.3466e-02, -6.5438e-02],\n",
       "        [-7.4270e-02,  1.1263e-02],\n",
       "        [-1.2326e-01, -9.1198e-02],\n",
       "        [-8.3829e-02, -5.1695e-02],\n",
       "        [-5.8353e-02, -9.5123e-02],\n",
       "        [-6.7198e-02, -5.4672e-02],\n",
       "        [-6.0789e-02, -6.5281e-02],\n",
       "        [-7.5243e-02, -6.0149e-02],\n",
       "        [-1.6797e-01,  1.3015e-02],\n",
       "        [-9.5700e-02, -1.8600e-02],\n",
       "        [-1.0503e-01, -9.1028e-03],\n",
       "        [-1.0011e-01, -7.5088e-02],\n",
       "        [-6.0191e-02, -1.0209e-01],\n",
       "        [-1.0375e-01, -1.1576e-01],\n",
       "        [-9.6413e-02, -1.9755e-02],\n",
       "        [-1.3959e-01, -4.9291e-02],\n",
       "        [-8.4051e-02, -5.4721e-02],\n",
       "        [-1.4399e-01, -3.8989e-02],\n",
       "        [-6.6585e-02, -1.0334e-02],\n",
       "        [-5.2417e-02,  2.1913e-03],\n",
       "        [-6.0618e-02, -7.2582e-02],\n",
       "        [-8.2682e-02, -6.5617e-02],\n",
       "        [-1.5064e-01,  4.1228e-02],\n",
       "        [-6.4130e-02,  2.9455e-03],\n",
       "        [-1.4089e-01, -3.5418e-02],\n",
       "        [-8.7067e-02, -3.8371e-02],\n",
       "        [-1.6747e-01, -7.1187e-02],\n",
       "        [-1.1812e-01, -2.8606e-02],\n",
       "        [-1.4587e-01, -5.1282e-02],\n",
       "        [-4.3804e-02, -3.1714e-02],\n",
       "        [-5.0148e-02, -1.0164e-01],\n",
       "        [-1.3913e-01, -9.0025e-02],\n",
       "        [-1.2928e-01, -5.8042e-02],\n",
       "        [-9.6077e-02, -7.5911e-02],\n",
       "        [-1.6631e-01, -2.9304e-02],\n",
       "        [-2.6296e-02, -3.7285e-02],\n",
       "        [-1.5005e-01, -8.5599e-02],\n",
       "        [-1.8002e-01,  6.8119e-02],\n",
       "        [-1.8029e-01, -9.3247e-02],\n",
       "        [-1.5581e-01,  1.1133e-02],\n",
       "        [-7.3129e-02, -6.7041e-02],\n",
       "        [-4.7570e-02, -1.9072e-02],\n",
       "        [-1.5782e-01, -1.1768e-01],\n",
       "        [-8.9564e-02, -9.1125e-02],\n",
       "        [-1.0543e-01,  5.9993e-02],\n",
       "        [-9.8331e-02, -4.6409e-02],\n",
       "        [-1.1168e-01, -5.7158e-02],\n",
       "        [-7.0228e-02, -6.3925e-02],\n",
       "        [-6.3776e-02, -1.4776e-02],\n",
       "        [-9.9354e-02, -9.1530e-03],\n",
       "        [-1.6239e-01,  4.6077e-02],\n",
       "        [-6.3963e-02, -3.1306e-02],\n",
       "        [-8.8866e-02,  2.2013e-02],\n",
       "        [-1.2702e-01,  6.2041e-03],\n",
       "        [-7.1872e-02, -2.0180e-02],\n",
       "        [-1.2019e-01, -4.2687e-02],\n",
       "        [-5.8592e-02,  7.6843e-02],\n",
       "        [-7.5140e-02, -9.6856e-02],\n",
       "        [-9.2649e-02, -2.5776e-02],\n",
       "        [-1.4422e-01, -5.4295e-02],\n",
       "        [-1.0332e-01, -4.0397e-02],\n",
       "        [-5.4688e-02, -5.0567e-02],\n",
       "        [-4.2066e-05,  7.4105e-02],\n",
       "        [-6.9445e-02, -2.9063e-02],\n",
       "        [-1.2383e-01, -1.0834e-01],\n",
       "        [-1.0182e-01, -8.0562e-02],\n",
       "        [-1.4562e-01, -1.3521e-01],\n",
       "        [-1.9760e-01,  2.3645e-02],\n",
       "        [-8.8233e-02, -6.3161e-02],\n",
       "        [-8.1542e-02, -9.3173e-02],\n",
       "        [-7.9674e-02, -1.5411e-02],\n",
       "        [-3.0852e-02,  1.5349e-02],\n",
       "        [-1.4859e-01, -1.0490e-01],\n",
       "        [-1.8403e-01, -9.6537e-02],\n",
       "        [-2.0180e-01, -1.5774e-02],\n",
       "        [-1.1664e-01, -9.7588e-02],\n",
       "        [-1.6118e-01, -1.3042e-01],\n",
       "        [-1.4310e-01, -1.0258e-01],\n",
       "        [-6.3961e-02, -2.5859e-02],\n",
       "        [-1.2897e-01, -2.8273e-02],\n",
       "        [-7.6332e-02, -2.3135e-02],\n",
       "        [-9.9348e-02,  1.2814e-02],\n",
       "        [-1.6589e-01, -3.3952e-02],\n",
       "        [-7.0236e-02,  8.4328e-03],\n",
       "        [ 5.5797e-03,  1.1305e-02],\n",
       "        [-6.9800e-02, -5.4238e-02],\n",
       "        [-1.2052e-01, -2.2650e-02],\n",
       "        [-6.3799e-02, -5.7308e-03],\n",
       "        [-1.6260e-01, -8.4367e-02],\n",
       "        [-4.3863e-02, -2.8005e-02],\n",
       "        [-5.7668e-02, -6.0683e-02],\n",
       "        [-8.6751e-02, -3.8573e-02],\n",
       "        [-9.5785e-02, -1.1802e-02],\n",
       "        [-5.9531e-03, -1.5339e-02],\n",
       "        [-7.2644e-02, -1.0184e-01],\n",
       "        [-1.6969e-01, -7.0126e-03]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('dagon')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a7b4e01e215f5e68a77fe71862d53fabbc6d54aee881e11b9a5e519ca5c461bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
